# NLP-hot

Даннй репозиторий содержит презентации и доклады с выступлений по теме NLP.

Ниже перечислены основные моменты текущей ситуации в данной сфере с соотвествующими ссылками.

В 90е годы основной архитектурой в NLP были RNN.

Настоящее время - это эпоха трансформеров (архитектура была придумана в 2017 году Google https://arxiv.org/abs/1706.03762). С 2021 года активно развивается направление генеративных языковых моделей (decoder-only трансформеров) 

Появились языковые модели уже просто гигантских размеров (н-р, PALM от Google на 540 миллиардов параметров)
У больших моделей (свыше примерно 100 млрд параметров) наблюдается, так называемая, emergent ability, то есть способность выполнять задачи, на которые данная модель не была обучена

Большие языковые модели дорого дообучать, в связи с этим появилось новое направление prompt engeeniring, то есть разработка "правильных" затравок
В свою очередь, в prompt engineering также появляются новые интересные направления, например, Chain-of-Thought https://arxiv.org/abs/2201.11903, https://arxiv.org/abs/2205.11916, https://arxiv.org/abs/2203.11171 .

Трансформеры - это основа базовых моделей, но чтобы сделать эти модели более "человечными" моделями-ассистентами требуется дообучение. Хорошо себя показала методика RLHF. Отличная лекция Андрея Карпаты на эту тему по ссылке https://www.youtube.com/watch?v=bZQun8Y4L2A , а также статьи: https://arxiv.org/abs/1707.06347 , https://arxiv.org/abs/2203.02155 . В настоящее время перспективно выглядит новый подход principle-driven self-ALIGN с минимальным вовлечением человека (так Dromedary удалось дообучить, используя менее чем 300 строк аннотаций, написанных людьми) https://arxiv.org/pdf/2305.03047.pdf.

В качестве тренда можно выделить мультимодальность, пример, GPT-4

Среди SOTA генеративных моделей (decoder-only) можно выделить следующие из закрытых: GPT-4 https://arxiv.org/abs/2303.08774, ChatGPT (GPT-3,5) https://arxiv.org/abs/2005.14165; из open source: LLama https://arxiv.org/abs/2302.13971, Alpaca https://github.com/tatsu-lab/stanford_alpaca/blob/main/README.md?ysclid=lja9f2lejr3199945 , Stable Vicuna https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot, Dromedary https://arxiv.org/pdf/2305.03047.pdf; от русскоязычных команд opensource: Yalm100B oт Яндекс, Saiga от команды Ильи Гусева https://github.com/IlyaGusev/rulm?ysclid=lja9j4jck8490934127 ; из корпоративных закрытых: GigaChat, YandexGPT. Также появляются модели-ассистенты под конкретные задачи, например, BloombergGPT. Среди моделей encoder-decoder: T5 и русскоязычные ruT5, FRED-T5. Среди encoder-only: семейство BERT.

Для обучения/дообучения больших языковых моделей недавно были изобретены новые оптимизаторы, которые превосходят по скорости схождения AdamW: это LION от Google https://arxiv.org/abs/2302.06675 и Sophya от Стэнфорда https://arxiv.org/pdf/2305.14342.pdf.

Однако, несмотря на существование таких мощных трансформеров, для разных задач в NLP разумно использовать разные инструменты или их комбинации. Например, если для задачи, в которой крайне важна точность и понимание, как именно действует алгоритм,  а также возможность поправить его "руками" при необходимости (например, для определенных NER-задач), то вероятно лучше подойдут регулярки. Для несложной классификации текста может лучше подойти TF-IDF логрегрессия. Для сложной классификации: encoder-only трансформер. Для творческих задач генерации текста, "общения": decoder-only трансформер.

В завершение можно коротко перечислить некоторые проблемы в данной сфере: модели могут "галлюцинировать"; модели ограничены данными, на которых они обучались; все чаще встает вопрос воспроизводимости результатов научных статей (т.к. новые методы часто предназначены для гигантских закрытых моделей); вопрос открытости новых решений; вопрос регулирования данных разработок со стороны общества и др.

